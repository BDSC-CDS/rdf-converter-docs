%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{RDF-i2b2-converter Documentation}
\date{Nov 08, 2022}
\release{1.0}
\author{Jules Fasquelle}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


The converter allows you to \sphinxstylestrong{turn a RDF knowledge graph into a set of CSV tables fitting the i2b2 common model}.
An \sphinxstylestrong{ontology} converter turns the RDF ontology into i2b2 metadata content (ONT cell tables).
A \sphinxstylestrong{data samples} converter turns the RDF instances into i2b2 star schema (CRC cell tables).
Both can be used independently, albeit comparing the consistency of their outputs is a good idea.

Docker configurations for both modules are available and described in the associated page of this documentation. Yet, you can also run the converter(s) from the Python source files.
The main advantage of user the Docker releases is easy input/output file management and little amount of user configuration.

\sphinxstylestrong{Quality assessment of the data samples conversion requires comparison against the converted ontology records.} It is then recommended to run the ontology conversion first.

\sphinxstylestrong{Conversion scenario (ontology or data samples conversion):}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
Clone the relevant repository (see {\hyperref[\detokenize{index:reqs}]{\sphinxcrossref{\DUrole{std,std-ref}{Requirements and installation}}}})

\item {} 
Arrange your input files and output folders according to the predefined structure (see {\hyperref[\detokenize{structure:structure}]{\sphinxcrossref{\DUrole{std,std-ref}{I/O folders structure}}}})

\item {} 
Modify the provided configuration files if necessary (see {\hyperref[\detokenize{index:index-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{Configuration}}}})

\item {} 
Run the converter ({\hyperref[\detokenize{run_data:run-data}]{\sphinxcrossref{\DUrole{std,std-ref}{data converter instructions}}}}, {\hyperref[\detokenize{run_ontology:run-ontology}]{\sphinxcrossref{\DUrole{std,std-ref}{ontology converter instructions}}}}) and check the results {\hyperref[\detokenize{verbose:verbose}]{\sphinxcrossref{\DUrole{std,std-ref}{Debug}}}} if necessary.

\end{enumerate}


\chapter{Requirements and installation}
\label{\detokenize{index:requirements-and-installation}}

\section{Using Docker}
\label{\detokenize{index:using-docker}}
\sphinxhref{https://github.com/CHUV-DS/docker-ontology-converter}{Repository for the ontology converter (Docker)}

\sphinxhref{https://github.com/CHUV-DS/docker-data-converter}{Repository for the data samples converter (Docker)}

To run the Dockerized converter(s) \sphinxstyleemphasis{smoothly}, you need to have installed
- Docker and \sphinxcode{\sphinxupquote{make}}
- Internet access from inside your container (automatic \sphinxcode{\sphinxupquote{git pull}} at container startup)
Then, please follow the instructions in the {\hyperref[\detokenize{docker:docker}]{\sphinxcrossref{\DUrole{std,std-ref}{dedicated page}}}}.

Exotic setups (Singularity, offline container, docker-compose instead of make, etc.) will also work if you know what you are doing (explore the Dockerfile and Makefile and adapt their content for your use).


\section{Using Python and shell source files}
\label{\detokenize{index:using-python-and-shell-source-files}}
\sphinxhref{https://github.com/CHUV-DS/RDF-i2b2-converter}{Repository of the source code}

To run the source files directly, you need to have installed
\begin{itemize}
\item {} 
Python \textgreater{}= 3.7

\item {} 
rdflib \textgreater{}= 6.0.2

\item {} 
pandas \textgreater{}= 1.3.4

\item {} 
psutils \textgreater{}= 5.9.2 (for memory monitoring)

\end{itemize}

In this case, follow the instructions of the {\hyperref[\detokenize{installation:installation}]{\sphinxcrossref{\DUrole{std,std-ref}{dedicated page}}}}.


\chapter{Configuration}
\label{\detokenize{index:configuration}}
The converter comes from three configuration files read by the source code. If using a container, the config directory should be mounted as a shared volume with your host (see how to do this in the {\hyperref[\detokenize{docker:makefile}]{\sphinxcrossref{\DUrole{std,std-ref}{Makefile section of the Docker deployment instructions}}}}).
All the details about every configuration file are described on {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{this page}}}}, either for ontology or data conversion.


\chapter{Verbose mode}
\label{\detokenize{index:verbose-mode}}
A \sphinxstyleemphasis{verbose} mode is available, which will output human-readable identifying codes for i2b2 concepts and modifiers, instead of hashes. This is useful to assert the ontology and data converter worked consistently (and also that the input RDF data is consistent with the input RDF ontology).
We recommend to start by a \sphinxstyleemphasis{verbose} run and read the run logs. See how to do it and interpret the results {\hyperref[\detokenize{verbose:verbose}]{\sphinxcrossref{\DUrole{std,std-ref}{in the dedicated page}}}}. For the data samples conversion, if you are happy with the logs, you will be able to convert the verbose output into production-ready tables easily.
You can also bypass the \sphinxstyleemphasis{verbose} run and issue directly your tables in production mode.


\section{Docker usage}
\label{\detokenize{docker:docker-usage}}\label{\detokenize{docker:docker}}\label{\detokenize{docker::doc}}
The two converters are available in dedicated docker releases and work using a Docker image (either precompiled or to build locally), and a set of Makefile recipes that will help you manage the container while mounting the necessary Docker volumes. The volumes are useful to bind files (config, data source, utility, etc.) from your host to the container at startup.


\subsection{Ontology converter docker}
\label{\detokenize{docker:ontology-converter-docker}}
The docker source files are available on \sphinxhref{https://github.com/CHUV-DS/docker-ontology-converter.git}{Github (ontology converter)}.
Make sure your folder structure is correct (see {\hyperref[\detokenize{structure:structure}]{\sphinxcrossref{\DUrole{std,std-ref}{the dedicated section}}}}) and then run the desired recipe from the Makefile.


\subsection{Data converter docker}
\label{\detokenize{docker:data-converter-docker}}
The docker source files are available on \sphinxhref{https://github.com/CHUV-DS/docker-data-converter.git}{Github (data converter)}.
Make sure your folder structure is correct (see {\hyperref[\detokenize{structure:structure}]{\sphinxcrossref{\DUrole{std,std-ref}{the dedicated section}}}}) and then run the desired recipe from
the Makefile.


\subsection{Installation using the Makefile}
\label{\detokenize{docker:installation-using-the-makefile}}\label{\detokenize{docker:makefile}}
Requirements: gcc make. If not available, simply execute the command lines that are in the Makefile, as they are very explicit.

The instructions in the Makefile will help you define the local variables pointing to your local directories.

Get the docker image by running

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{make build}
\end{sphinxVerbatim}

Run the container by executing either

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{make up\PYGZhy{}d}
\PYG{g+gp}{\PYGZsh{}} Or
\PYG{g+go}{make debug}
\end{sphinxVerbatim}


\subsection{Common fails at runtime:}
\label{\detokenize{docker:common-fails-at-runtime}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{pop from empty list}
\end{sphinxVerbatim}

This one means your source RDF files were not found. Check the environment variables defined in the Makefile.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{Conversion passed but consistency check were not performed due to absence of ontology tables (CONCEPT\PYGZus{}DIMENSION and/or MODIFIER\PYGZus{}DIMENSION in the folder)}
\end{sphinxVerbatim}

Check the CONCEPT\_DIMENSION and MODIFIER\_DIMENSION (with trailing “\_VERBOSE” if verbose mode) are in the destination folder. Check ( \sphinxcode{\sphinxupquote{\$ head -1 \$file}} )  the column labels are upper-case.


\section{Installation of the RDF-i2b2 converter from Python source files}
\label{\detokenize{installation:installation-of-the-rdf-i2b2-converter-from-python-source-files}}\label{\detokenize{installation:installation}}\label{\detokenize{installation::doc}}
This is the documentation page for the installation of the RDF-i2b2 converter \sphinxstylestrong{from the source files}. It is the recommended approach if you are a developer and you want to adapt the code to your specific RDF or output design. It is also the way to go if you do not want to use Docker or any other related system.

You will need to have Python 3.7 or more recent installed, with the following dependencies:

Then, you need to \sphinxstylestrong{modify} the src/utils.py file so the three variables GRAPH\_CONFIG, DATA\_CONFIG, I2B2\_MAPPING match the paths to your config files. Example config files are provided in the config/ folder at the root of the repository. To use them as is, simply change \sphinxstyleemphasis{/config/} for “\sphinxstyleemphasis{local\_config\_templates/}


\section{Organization of the input/output files and folders}
\label{\detokenize{structure:organization-of-the-input-output-files-and-folders}}\label{\detokenize{structure:structure}}\label{\detokenize{structure::doc}}
The preferred approach is to have two folders, one which will be used as output for the verbose mode, and one for the production mode.
The input (RDF graph) files should all be descendants of a unique folder. The converter will load incrementally in a single graph all .ttl files found below the starting point (exploring recursively the subfolders and following symlinks). Thus, you should remove the undesired graphs or zip them.

{\color{red}\bfseries{}\textbar{}}— verbose folder
\textbar{}           \textbar{}
\textbar{}            —- CONCEPT\_DIMENSION\_VERBOSE.csv
\textbar{}            —- MODIFIER\_DIMENSION\_VERBOSE.csv
\textbar{}            —- migrations\_logs.csv
…
\textbar{}
{\color{red}\bfseries{}\textbar{}}— output folder
\textbar{}           \textbar{}
\textbar{}            —- CONCEPT\_DIMENSION.csv
\textbar{}            —- MODIFIER\_DIMENSION.csv
\textbar{}            —- METADATA.csv
\textbar{}            —- TABLE\_ACCESS.csv
\textbar{}            —- migrations\_logs.csv
\textbar{}            —- OBSERVATION\_FACT.csv
\textbar{}            —- … \textless{}5 other CSV tables (i2b2 star schema)\textgreater{}
…
\textbar{}
{\color{red}\bfseries{}\textbar{}}—   graphs
\textbar{}           \textbar{}
\textbar{}            —- first cohort
\begin{quote}

\begin{DUlineblock}{0em}
\item[] {\color{red}\bfseries{}\textbar{}}
\item[] —- data\_graph\_1.ttl
\item[] —- data\_graph\_1b.ttl
\item[] —- second cohort
\item[] {\color{red}\bfseries{}\textbar{}}
\item[] — \textless{}…\textgreater{}
\item[] 
\item[] —- any\_graph.ttl
\end{DUlineblock}
\end{quote}


\section{Organization of the config files}
\label{\detokenize{configuration:organization-of-the-config-files}}\label{\detokenize{configuration:configuration}}\label{\detokenize{configuration::doc}}

\subsection{graph\_config.json}
\label{\detokenize{configuration:graph-config-json}}
This configuration file helps piloting the RDF graphs parsing and discovery. It is used by both the ontology converter and the data converter. It defines the RDF terms to be expected in the graph, the reserved URIs, etc. It also features a lookup table for terminology RDF graphs, so they can be loaded by the ontology converter in a distinct memory slot as the main ontology graph, speeding up the computations.
The fields are as follow:
.. csv-table:: Field description of graph\_config
\begin{quote}
\begin{quote}\begin{description}
\item[{file}] \leavevmode
graph\_config\_doc.csv

\item[{widths}] \leavevmode
30, 70

\item[{header-rows}] \leavevmode
1

\end{description}\end{quote}
\end{quote}


\subsection{data\_config.json}
\label{\detokenize{configuration:data-config-json}}
This file is only used by the data converter.
It therefore specifies a data-specific blacklist, the path to the data and dependencies graphs, and describes the mappings for RDF contextual fields that are not featured in the i2b2 ontology.
It contains structured instructions about how these fields should be unpacked and mapped to specific table and columns of the i2b2 star schema.
.. csv-table:: Field description of graph\_config
\begin{quote}
\begin{quote}\begin{description}
\item[{file}] \leavevmode
data\_config\_doc.csv

\item[{widths}] \leavevmode
30, 70

\item[{header-rows}] \leavevmode
1

\end{description}\end{quote}
\end{quote}


\subsection{i2b2\_rdf\_config.json}
\label{\detokenize{configuration:i2b2-rdf-config-json}}
This file describes the bindings between RDF datatypes and i2b2 table-columns, and defines additional filters for ontology elements that should or should not appear in the final ontology.
.. csv-table:: Field description of i2b2\_rdf\_config
\begin{quote}
\begin{quote}\begin{description}
\item[{file}] \leavevmode
i2b2\_config\_doc.csv

\item[{widths}] \leavevmode
30, 70

\item[{header-rows}] \leavevmode
1

\end{description}\end{quote}
\end{quote}


\section{Verbose mode explained}
\label{\detokenize{verbose:verbose-mode-explained}}\label{\detokenize{verbose:verbose}}\label{\detokenize{verbose::doc}}
Two modes are available for each converter: \sphinxstyleemphasis{verbose} and \sphinxstyleemphasis{regular}. The output of a \sphinxstyleemphasis{verbose} ontology conversion is compatible with the output of a \sphinxstyleemphasis{verbose} data conversion, and same for the regular mode.
\sphinxstylestrong{IMPORTANT:} There is no point running your data conversion in \sphinxstyleemphasis{verbose} mode if you don’t have access to the ontology tables in \sphinxstyleemphasis{verbose} mode as well (\sphinxstyleemphasis{CONCEPT\_DIMENSION.csv} and \sphinxstyleemphasis{MODIFIER\_DIMENSION.csv} are enough)

Background knowledge: i2b2 features a \sphinxstylestrong{basecode} that allows to join the ontology concepts (metadata) with the observation records. This basecode is independently but deterministically generated by both the converters (ontology and data). Both basecode sets should match. Mismatches are equivalent to ignored data records (since i2b2 cannot identify the concept).
\begin{itemize}
\item {} 
Generating tables in \sphinxstyleemphasis{verbose} mode allows you to take a look at the generated code and see if they look legit

\item {} 
Since i2b2 needs capped-length codes to work, we use a recursive hash to generate the production-ready tables.

\end{itemize}


\subsection{Setting up for verbose mode}
\label{\detokenize{verbose:setting-up-for-verbose-mode}}
In the Docker setup, the {\hyperref[\detokenize{docker:makefile}]{\sphinxcrossref{\DUrole{std,std-ref}{Makefile}}}} allows you to specify a distinct target directory for verbose and regular modes, and to trigger each mode with a different recipe.
If running from the source files, you should either modify the \sphinxstyleemphasis{OUTPUT\_TABLES\_LOCATION} variables to make it point to a directory where you want your verbose tables to be written/read ; you should also manually switch the \sphinxstyleemphasis{DEBUG} config variable to \sphinxstylestrong{“True”} in the \sphinxstyleemphasis{i2b2\_rdf\_config.json} file. Then, start the conversion normally.


\subsection{Interpreting the results of verbose run}
\label{\detokenize{verbose:interpreting-the-results-of-verbose-run}}
If you configured correctly your I/O and config files, the program (either on verbose or regular mode)  will print (last line of execution) one of the following messages:
- \sphinxstyleemphasis{Success! All items are consistent with the ontology.}
- \sphinxstyleemphasis{Some concepts or modifiers are not in the ontology. Please take a look at the “logs\_missing\_concepts” and “logs\_missing\_modifiers” logfiles. If unreadable, change the “DEBUG” variable in the config files to True, and run the “make verbose” command.}

First case, you are good.
Second case, two logfiles were created and you should look into them and try to understand why they do not match ontology items. It could be that the RDF structure is different, or that the data contains codes that are not part of the ontology, etc. Based on this knowledge, you can either correct your data and convert it again (or correct in-place the CSV if the issue is a minor typo), or keep going (but don’t forget all records featuring codes that are in the logfiles will be ignored by the i2b2 query system).


\subsection{Turn the \sphinxstyleemphasis{verbose} tables into production-ready tables}
\label{\detokenize{verbose:turn-the-verbose-tables-into-production-ready-tables}}
A bash script will do it for you. After running the data conversion in \sphinxstyleemphasis{verbose} mode, you will find the script \sphinxstyleemphasis{postprod.bash} in the same folder where your \sphinxstyleemphasis{verbose} tables were written. Pass as named argument \sphinxstyleemphasis{-outputF} the folder in which your production ontology tables are, which is also where the production data tables will be written (optionnally, use the named argument \sphinxstyleemphasis{-verboseF} to specify the source: default setting is where the script is located.)


\section{Fast-forward: convert RDF data samples}
\label{\detokenize{run_data:fast-forward-convert-rdf-data-samples}}\label{\detokenize{run_data:run-data}}\label{\detokenize{run_data::doc}}

\subsection{With docker}
\label{\detokenize{run_data:with-docker}}
Once your docker image is installed (see {\hyperref[\detokenize{installation:installation}]{\sphinxcrossref{\DUrole{std,std-ref}{the installation section}}}}, your {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{configuration}}}} is ready and the {\hyperref[\detokenize{structure:structure}]{\sphinxcrossref{\DUrole{std,std-ref}{I/O folders}}}} are populated with the ontology tables, run:
\sphinxSetupCaptionForVerbatim{Example using a direct production run}
\def\sphinxLiteralBlockLabel{\label{\detokenize{run_data:id1}}}
\fvset{hllines={, 5,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} ls /opt/production\PYGZus{}tables
        MODIFIER\PYGZus{}DIMENSION.csv CONCEPT\PYGZus{}DIMENSION.csv METADATA.csv TABLE\PYGZus{}ACCESS.csv

\PYG{c+c1}{\PYGZsh{} Run the converter in production mode}
\PYGZdl{} make up

\PYG{c+c1}{\PYGZsh{} Or use \PYGZdl{}(make up\PYGZhy{}d) to avoid capturing the console}

\PYG{c+c1}{\PYGZsh{} Check the output}
\PYGZdl{} ls /opt/production\PYGZus{}tables
        MODIFIER\PYGZus{}DIMENSION.csv CONCEPT\PYGZus{}DIMENSION.csv METADATA.csv TABLE\PYGZus{}ACCESS.csv
        OBSERVATION\PYGZus{}FACT.csv PROVIDER\PYGZus{}DIMENSION.CSV PATIENT\PYGZus{}DIMENSION.CSV PATIENT\PYGZus{}MAPPING.CSV
        VISIT\PYGZus{}DIMENSION.CSV ENCOUNTER\PYGZus{}MAPPING.CSV
\end{sphinxVerbatim}
\sphinxresetverbatimhllines

And for a verbose run:
\sphinxSetupCaptionForVerbatim{Example using a verbose run}
\def\sphinxLiteralBlockLabel{\label{\detokenize{run_data:id2}}}
\fvset{hllines={, 8, 15,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Check the I/O folders
\PYGZdl{} ls /opt/verbose\PYGZus{}tables
        MODIFIER\PYGZus{}DIMENSION\PYGZus{}DEBUG.csv CONCEPT\PYGZus{}DIMENSION\PYGZus{}DEBUG.csv
\PYGZdl{} ls /opt/production\PYGZus{}tables
        MODIFIER\PYGZus{}DIMENSION.csv CONCEPT\PYGZus{}DIMENSION.csv METADATA.csv TABLE\PYGZus{}ACCESS.csv

\PYGZsh{} Run the converter in verbose mode
\PYGZdl{} make verbose DEBUG\PYGZus{}FOLDER=/opt/verbose\PYGZus{}tables

\PYGZdl{} cd /opt/verbose\PYGZus{}tables
\PYGZdl{} cat logs\PYGZus{}missing\PYGZus{}modifiers.csv
        ...\PYGZlt{}list of data items that don\PYGZsq{}t match ontology items\PYGZgt{}...
\PYGZsh{} If I\PYGZsq{}m happy with this (or file doesn\PYGZsq{}t exist), I can move on and use this for production

\PYGZdl{} bash postprod.bash \PYGZhy{}verboseF /opt/verbose\PYGZus{}tables \PYGZhy{}outputF /opt/production\PYGZus{}tables

\PYGZsh{} Check the output
\PYGZdl{} ls /opt/production\PYGZus{}tables
       MODIFIER\PYGZus{}DIMENSION.csv CONCEPT\PYGZus{}DIMENSION.csv
       METADATA.csv TABLE\PYGZus{}ACCESS.csv
       OBSERVATION\PYGZus{}FACT.csv PROVIDER\PYGZus{}DIMENSION.CSV
       PATIENT\PYGZus{}DIMENSION.CSV PATIENT\PYGZus{}MAPPING.CSV
       VISIT\PYGZus{}DIMENSION.CSV ENCOUNTER\PYGZus{}MAPPING.CSV
\end{sphinxVerbatim}
\sphinxresetverbatimhllines


\subsection{Without Docker}
\label{\detokenize{run_data:without-docker}}
The steps are roughly the same (here shown only for the verbose scenario). Only make sure your configuration files point to the correct folder (for this example, the \sphinxstyleemphasis{/opt/verbose\_tables/} folder and have the correct value for the \sphinxstyleemphasis{DEBUG} variable (\sphinxstyleemphasis{True} for this example):
\sphinxSetupCaptionForVerbatim{Example using a verbose run}
\def\sphinxLiteralBlockLabel{\label{\detokenize{run_data:id3}}}
\fvset{hllines={, 9, 16,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Check the I/O folders
\PYGZdl{} ls /opt/verbose\PYGZus{}tables
        MODIFIER\PYGZus{}DIMENSION\PYGZus{}DEBUG.csv CONCEPT\PYGZus{}DIMENSION\PYGZus{}DEBUG.csv
\PYGZdl{} ls /opt/production\PYGZus{}tables
        MODIFIER\PYGZus{}DIMENSION.csv CONCEPT\PYGZus{}DIMENSION.csv
        METADATA.csv TABLE\PYGZus{}ACCESS.csv

\PYGZsh{} Run the converter in verbose mode
\PYGZdl{} cd RDF\PYGZhy{}i2b2\PYGZhy{}converter
\PYGZdl{} python3 src/main\PYGZus{}data.py

\PYGZdl{} cd /opt/verbose\PYGZus{}tables
\PYGZdl{} cat logs\PYGZus{}missing\PYGZus{}modifiers.csv
        ...\PYGZlt{}list of data items that don\PYGZsq{}t match ontology items\PYGZgt{}...
\PYGZsh{} If I\PYGZsq{}m happy with this (or file doesn\PYGZsq{}t exist),
\PYGZsh{} I can move on and use this for production

\PYGZdl{} bash postprod.bash \PYGZhy{}verboseF /opt/verbose\PYGZus{}tables \PYGZhy{}outputF /opt/production\PYGZus{}tables

\PYGZsh{} Check the output
\PYGZdl{} ls /opt/production\PYGZus{}tables
        MODIFIER\PYGZus{}DIMENSION.csv CONCEPT\PYGZus{}DIMENSION.csv
        METADATA.csv TABLE\PYGZus{}ACCESS.csv
        OBSERVATION\PYGZus{}FACT.csv PROVIDER\PYGZus{}DIMENSION.CSV
        PATIENT\PYGZus{}DIMENSION.CSV PATIENT\PYGZus{}MAPPING.CSV
        VISIT\PYGZus{}DIMENSION.CSV ENCOUNTER\PYGZus{}MAPPING.CSV
\end{sphinxVerbatim}
\sphinxresetverbatimhllines


\section{Instructions to run the ontology converter}
\label{\detokenize{run_ontology:instructions-to-run-the-ontology-converter}}\label{\detokenize{run_ontology:run-ontology}}\label{\detokenize{run_ontology::doc}}
Once your docker image is installed (see {\hyperref[\detokenize{installation:installation}]{\sphinxcrossref{\DUrole{std,std-ref}{the installation section}}}}, your {\hyperref[\detokenize{configuration:configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{configuration}}}} is ready and the {\hyperref[\detokenize{structure:structure}]{\sphinxcrossref{\DUrole{std,std-ref}{I/O folders}}}} are created and populated by optional instruction files:
\sphinxSetupCaptionForVerbatim{Example using a direct production run}
\def\sphinxLiteralBlockLabel{\label{\detokenize{run_ontology:id1}}}
\fvset{hllines={, 5,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} ls /opt/production\PYGZus{}tables
       lookup\PYGZus{}units.csv

\PYG{c+c1}{\PYGZsh{} Run the converter in production mode}
\PYGZdl{} make up

\PYG{c+c1}{\PYGZsh{} Or use \PYGZdl{}(make up\PYGZhy{}d) to avoid capturing the console}

\PYG{c+c1}{\PYGZsh{} Check the output}
\PYGZdl{} ls /opt/production\PYGZus{}tables
       MODIFIER\PYGZus{}DIMENSION.csv CONCEPT\PYGZus{}DIMENSION.csv
       METADATA.csv TABLE\PYGZus{}ACCESS.csv
       lookup\PYGZus{}units.csv \PYG{o}{(}migrations\PYGZus{}logs.csv\PYG{o}{)}
\end{sphinxVerbatim}
\sphinxresetverbatimhllines
\sphinxSetupCaptionForVerbatim{Example using a verbose run}
\def\sphinxLiteralBlockLabel{\label{\detokenize{run_ontology:id2}}}
\fvset{hllines={, 6,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check the I/O folders}
\PYGZdl{} ls /opt/verbose\PYGZus{}tables
       lookup\PYGZus{}units.csv

\PYG{c+c1}{\PYGZsh{} Run the converter in verbose mode}
\PYGZdl{} make verbose \PYG{n+nv}{DEBUG\PYGZus{}FOLDER}\PYG{o}{=}/opt/verbose\PYGZus{}tables \PYG{n+nv}{ONTOLOGY\PYGZus{}LOCATION}\PYG{o}{=}/opt/ontology\PYGZus{}rdf\PYGZus{}graphs

\PYG{c+c1}{\PYGZsh{} Check the output}
\PYGZdl{} ls /opt/verbose\PYGZus{}tables
       MODIFIER\PYGZus{}DIMENSION\PYGZus{}VERBOSE.csv CONCEPT\PYGZus{}DIMENSION\PYGZus{}VERBOSE.csv
       METADATA.csv TABLE\PYGZus{}ACCESS.csv
       lookup\PYGZus{}units.csv \PYG{o}{(}migrations\PYGZus{}logs.csv\PYG{o}{)}
\end{sphinxVerbatim}
\sphinxresetverbatimhllines
\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}